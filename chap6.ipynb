{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n",
      "[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n",
      "[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n",
      "[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n",
      "[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n",
      "[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def ngrams(sentence, n):\n",
    "    words = sentence.split()\n",
    "    ngrams = zip(*[words[i:] for i in range(n)])\n",
    "    return list(ngrams)\n",
    "\n",
    "sentence = \"안녕하세요 만나서 진심으로 반가워요\"\n",
    "\n",
    "unigram = ngrams(sentence, 1)\n",
    "bigram = ngrams(sentence, 2)\n",
    "trigram = ngrams(sentence, 3)\n",
    "\n",
    "print(unigram)\n",
    "print(bigram)\n",
    "print(trigram)\n",
    "\n",
    "unigram = nltk.ngrams(sentence.split(), 1)\n",
    "bigram = nltk.ngrams(sentence.split(), 2)\n",
    "trigram = nltk.ngrams(sentence.split(), 3)\n",
    "\n",
    "print(list(unigram))\n",
    "print(list(bigram))\n",
    "print(list(trigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF 클래스\n",
    "# tfidf_vertorizer = sklearn.feature_extraction.text.TfidVectorizer(\n",
    "#     input = \"content\",\n",
    "#     encoding = \"uft-8\",\n",
    "#     lowercase = True,\n",
    "#     stop_words = None,\n",
    "#     ngram_range = (1,1),\n",
    "#     max_df = 1.0,\n",
    "#     min_df = 1,\n",
    "#     vocabulary = None,\n",
    "#     smooth_idf = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.39687454 0.39687454 0.         0.79374908\n",
      "  0.2344005 ]\n",
      " [0.61980538 0.         0.         0.         0.61980538 0.\n",
      "  0.48133417]\n",
      " [0.4804584  0.63174505 0.         0.         0.4804584  0.\n",
      "  0.37311881]]\n",
      "{'that': 6, 'movie': 5, 'is': 3, 'famous': 2, 'like': 4, 'actor': 0, 'don': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"That movie is famous movie\",\n",
    "          \"I like that actor\",\n",
    "          \"I don't like that actor\"]\n",
    "\n",
    "tfidf_vertorizer = TfidfVectorizer()\n",
    "tfidf_vertorizer.fit(corpus)\n",
    "tfidf_matrix = tfidf_vertorizer.transform(corpus)\n",
    "\n",
    "print(tfidf_matrix.toarray())\n",
    "print(tfidf_vertorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/yeeun/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/yeeun/Korpora/nsmc/ratings_test.txt\n",
      "[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "class VanillaSkipgram(nn.Module) : \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim = embedding_dim\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features = embedding_dim,\n",
    "            out_features = vocab_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        output = self.linear(embeddings)\n",
    "        return output\n",
    "    \n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus = pd.DataFrame(corpus.test)\n",
    "\n",
    "tokenizer = Okt()\n",
    "tokens = [tokenizer.morphs(review) for review in corpus.text]\n",
    "print(tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '.', '이', '영화', '의', '..', '가', '에', '...', '을']\n",
      "5001\n"
     ]
    }
   ],
   "source": [
    "#단어 사전 구축\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n",
    "token_to_id = {token : idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx : token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['굳', 'ㅋ'], ['ㅋ', '굳'], ['뭐', '야'], ['뭐', '이'], ['야', '뭐']]\n"
     ]
    }
   ],
   "source": [
    "def get_word_pairs(tokens, window_size):\n",
    "    pairs = []\n",
    "    for sentence in tokens:\n",
    "        sentence_length = len(sentence)\n",
    "        for idx, center_word in enumerate(sentence):\n",
    "            window_start = max(0, idx - window_size)\n",
    "            window_end = min(sentence_length, idx + window_size + 1)\n",
    "            center_word = sentence[idx]\n",
    "            context_words = sentence[window_start : idx] + sentence[idx+1:window_end]\n",
    "            for context_word in context_words:\n",
    "                pairs.append([center_word, context_word])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "word_pairs = get_word_pairs(tokens, window_size=2)\n",
    "print(word_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[595, 100], [100, 595], [77, 176], [77, 2], [176, 77]]\n"
     ]
    }
   ],
   "source": [
    "def get_index_pairs(word_pairs, token_to_id):\n",
    "    pairs = []\n",
    "    unk_index = token_to_id[\"<unk>\"]\n",
    "    for word_pair in word_pairs:\n",
    "        center_word, context_word = word_pair\n",
    "        center_index = token_to_id.get(center_word, unk_index)\n",
    "        context_index = token_to_id.get(context_word, unk_index)\n",
    "        pairs.append([center_index, context_index])\n",
    "    return pairs\n",
    "\n",
    "index_pairs = get_index_pairs(word_pairs, token_to_id)\n",
    "print(index_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "index_pairs = torch.tensor(index_pairs)\n",
    "center_indexs = index_pairs[:,0]\n",
    "context_indexs = index_pairs[:,1]\n",
    "\n",
    "dataset = TensorDataset(center_indexs, context_indexs)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "word2vec = VanillaSkipgram(vocab_size = len(token_to_id), embedding_dim=128).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(word2vec.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1, Cost : 6.197\n",
      "Epoch :    2, Cost : 5.982\n",
      "Epoch :    3, Cost : 5.932\n",
      "Epoch :    4, Cost : 5.902\n",
      "Epoch :    5, Cost : 5.879\n",
      "Epoch :    6, Cost : 5.861\n",
      "Epoch :    7, Cost : 5.847\n",
      "Epoch :    8, Cost : 5.834\n",
      "Epoch :    9, Cost : 5.822\n",
      "Epoch :   10, Cost : 5.812\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    cost = 0.0\n",
    "    for input_ids, target_ids in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "\n",
    "        logits = word2vec(input_ids)\n",
    "        loss = criterion(logits, target_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += loss\n",
    "    cost = cost/len(dataloader)\n",
    "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연기\n",
      "[ 0.72424287  1.0915755  -1.587644   -0.3491957  -0.75607747 -0.9597175\n",
      " -0.47101802 -0.11625759 -0.27915177  0.06720276 -0.68313175  1.0592562\n",
      "  0.48842704 -0.47837684 -0.2680887   2.0928686  -1.7223847  -0.17120317\n",
      "  0.01181037  1.0288188   0.7636401   0.6241336  -0.10241289  0.2689511\n",
      " -0.6385944   1.3816987  -0.35228115 -1.0484514   0.44408992  0.10753936\n",
      " -0.15261498  2.7125964  -0.90335494 -0.69242    -0.47832274  0.36148518\n",
      " -0.47533378 -0.54475045 -1.8476114  -1.0700921  -1.1478374   0.7126867\n",
      " -0.74535424  0.7607283   1.2522407   0.60085213  0.9645007   1.1789963\n",
      " -0.6707039  -0.3105997   0.2884599  -0.3645838  -0.02725351  0.20753634\n",
      "  1.3472162   0.0555011  -0.03244544 -1.9904413   0.24200493 -1.2087156\n",
      "  2.3650115  -2.1402402   0.58521044  0.26125795  0.38241255  1.3344936\n",
      "  0.4493104  -1.4930584   1.6031467   0.40010494 -0.33559972 -1.2079908\n",
      " -1.3895787   0.5701243  -0.5068303  -0.02952182 -1.3210509   0.7895496\n",
      " -1.2478292  -1.0000788  -0.8724082  -0.2458829  -0.16017061  1.0272138\n",
      "  1.2447016  -0.2148527  -1.2653555  -1.4089323  -0.46186292 -0.57963246\n",
      " -0.39205477 -0.40935126 -1.7108285  -0.68158823  1.3507614  -0.5508072\n",
      " -0.29656112 -0.16451736  0.60276526  0.7923641   0.8633132  -0.18573955\n",
      "  1.1985999   0.6045537  -1.197396   -1.0903686   1.4780773  -0.6897198\n",
      "  0.18831833  0.12371892  0.49459848  1.3535371   0.82465816  1.0909455\n",
      "  0.7900822  -0.07281242  0.75399673 -0.6556189  -0.33609092 -0.7183449\n",
      " -0.29520217  0.68121356 -0.9879837  -0.6448444  -0.85775363  0.6376987\n",
      "  1.0314758  -0.7908742 ]\n"
     ]
    }
   ],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, embedding in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = embedding\n",
    "\n",
    "index = 30\n",
    "token = vocab[30]\n",
    "token_embedding = token_to_embedding[token]\n",
    "print(token)\n",
    "print(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연기와 가장 유사한 5개 단어\n",
      "캐릭터 - 유사도 : 0.3104\n",
      "시나리오 - 유사도 : 0.3040\n",
      "보네 - 유사도 : 0.2791\n",
      "차 - 유사도 : 0.2736\n",
      "권상우 - 유사도 : 0.2735\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    cosine = np.dot(b,a) / (norm(b, axis=1) * norm(a))\n",
    "    return cosine\n",
    "\n",
    "def top_n_index(cosine_matrix, n):\n",
    "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
    "    top_n = closest_indexes[1:n+1]\n",
    "    return top_n\n",
    "\n",
    "\n",
    "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
    "top_n = top_n_index(cosine_matrix, n=5)\n",
    "\n",
    "print(f\"{token}와 가장 유사한 5개 단어\")\n",
    "for index in top_n:\n",
    "    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2629403e-01 -4.2876619e-01  4.2294246e-01  2.7637497e-01\n",
      " -8.1281727e-03 -1.4619721e-01 -1.0980621e-01 -2.1422115e-01\n",
      " -6.1005408e-01  3.3167902e-01 -4.9763188e-02 -3.2741323e-01\n",
      " -1.2191890e-01  3.9545042e-04  3.4103815e-03 -5.1310893e-02\n",
      " -1.1330769e-01  1.9255647e-01  1.1238786e-02  1.3327871e-01\n",
      "  5.7884729e-01  2.0964241e-01 -1.8174107e-01  1.3611282e-01\n",
      " -1.3594410e-01 -9.0037495e-02 -2.1182044e-01 -3.5933509e-02\n",
      "  1.9990668e-01 -5.8824539e-02 -3.2459810e-01  1.6501996e-01\n",
      "  4.3267021e-01 -1.1745348e-02 -3.7856806e-02  1.3521846e-01\n",
      "  1.6099052e-01 -1.8145652e-01 -5.9838141e-03 -1.7579685e-01\n",
      "  9.2546165e-02  1.8764940e-01 -1.5325252e-02 -4.2333874e-01\n",
      " -4.5309034e-01  8.7818287e-02 -5.9380668e-01  9.6057832e-02\n",
      "  6.5749846e-02  6.5574534e-02  6.0589254e-01  1.0880921e-01\n",
      "  2.6193729e-01  3.2693529e-01 -2.7823395e-01 -3.2807904e-01\n",
      "  3.5826970e-02  1.3252126e-01 -2.7316359e-01  5.9432792e-03\n",
      " -1.6470324e-01 -1.2301714e-01  2.0437947e-01  3.5078311e-01\n",
      " -4.8017114e-01  7.6206908e-02  9.9648267e-02  2.8797555e-01\n",
      "  3.5917348e-01 -2.8257599e-01 -2.5039521e-01 -3.1118742e-01\n",
      " -5.2924037e-01  1.5483724e-01 -1.1686523e-01 -1.5766773e-01\n",
      " -1.9769849e-01 -3.6935058e-01 -6.8189964e-02  9.3497604e-02\n",
      " -3.1731388e-01  2.7655872e-02  4.1819394e-01  4.8446852e-01\n",
      "  4.4344682e-01 -3.9889753e-02  3.9037114e-01 -5.0667042e-01\n",
      "  6.2657259e-02  5.1300175e-02 -1.3686058e-01  7.8326978e-02\n",
      " -8.6471373e-03  1.5096745e-01  3.2589445e-01  1.2275755e-01\n",
      " -9.7897217e-02 -2.4938834e-01 -3.0444169e-01 -6.6752511e-01\n",
      " -4.0145585e-01 -2.2028551e-01  9.9881575e-02 -5.6188661e-01\n",
      "  1.7702843e-01  3.2429901e-01  5.6564891e-01  1.7239761e-01\n",
      "  7.2919637e-02 -2.8978771e-01  4.9175158e-01 -4.3840042e-01\n",
      " -2.3259108e-01  2.6386893e-01 -1.2748963e-03 -2.9670238e-01\n",
      "  6.2887591e-01  2.9538900e-01 -6.4270981e-02  3.4859195e-01\n",
      " -1.5258600e-01 -4.4812125e-01  2.7073002e-01 -2.7798956e-02\n",
      "  5.5799391e-02  3.4319464e-02 -4.7668388e-01 -9.6286364e-02]\n",
      "[('연기력', 0.7760466933250427), ('캐스팅', 0.7442580461502075), ('조연', 0.7385459542274475), ('몸매', 0.7281667590141296), ('목소리', 0.7243146896362305)]\n",
      "0.77604675\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "   sentences = tokens,\n",
    "   vector_size=128,\n",
    "   window=5,\n",
    "   min_count=1,\n",
    "   sg=1,\n",
    "   epochs=3,\n",
    "   max_final_vocab=10000\n",
    ")\n",
    "\n",
    "word = \"연기\"\n",
    "print(word2vec.wv[word])\n",
    "print(word2vec.wv.most_similar(word, topn=5))\n",
    "print(word2vec.wv.similarity(w1=word, w2=\"연기력\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : KakaoBrain\n",
      "    Repository : https://github.com/kakaobrain/KorNLUDatasets\n",
      "    References :\n",
      "        - Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). KorNLI and KorSTS: New Benchmark\n",
      "           Datasets for Korean Natural Language Understanding. arXiv preprint arXiv:2004.03289.\n",
      "           (https://arxiv.org/abs/2004.03289)\n",
      "\n",
      "    This is the dataset repository for our paper\n",
      "    \"KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding.\"\n",
      "    (https://arxiv.org/abs/2004.03289)\n",
      "    We introduce KorNLI and KorSTS, which are NLI and STS datasets in Korean.\n",
      "\n",
      "    # License\n",
      "    Creative Commons Attribution-ShareAlike license (CC BY-SA 4.0)\n",
      "    Details in https://creativecommons.org/licenses/by-sa/4.0/\n",
      "\n",
      "[Korpora] Corpus `kornli` is already installed at /Users/yeeun/Korpora/kornli/multinli.train.ko.tsv\n",
      "[Korpora] Corpus `kornli` is already installed at /Users/yeeun/Korpora/kornli/snli_1.0_train.ko.tsv\n",
      "[Korpora] Corpus `kornli` is already installed at /Users/yeeun/Korpora/kornli/xnli.dev.ko.tsv\n",
      "[Korpora] Corpus `kornli` is already installed at /Users/yeeun/Korpora/kornli/xnli.test.ko.tsv\n",
      "[['개념적으로', '크림', '스키밍은', '제품과', '지리라는', '두', '가지', '기본', '차원을', '가지고', '있다.'], ['시즌', '중에', '알고', '있는', '거', '알아?', '네', '레벨에서', '다음', '레벨로', '잃어버리는', '거야', '브레이브스가', '모팀을', '떠올리기로', '결정하면', '브레이브스가', '트리플', 'A에서', '한', '남자를', '떠올리기로', '결정하면', '더블', 'A가', '그를', '대신하러', '올라가고', 'A', '한', '명이', '그를', '대신하러', '올라간다.'], ['우리', '번호', '중', '하나가', '당신의', '지시를', '세밀하게', '수행할', '것이다.']]\n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "\n",
    "corpus = Korpora.load(\"kornli\")\n",
    "corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()\n",
    "tokens = [sentence.split() for sentence in corpus_texts]\n",
    "\n",
    "print(tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('개념적으로 크림 스키밍은 제품과 지리라는 두 가지 기본 차원을 가지고 있다.', '시즌 중에 알고 있는 거 알아? 네 레벨에서 다음 레벨로 잃어버리는 거야 브레이브스가 모팀을 떠올리기로 결정하면 브레이브스가 트리플 A에서 한 남자를 떠올리기로 결정하면 더블 A가 그를 대신하러 올라가고 A 한 명이 그를 대신하러 올라간다.', '우리 번호 중 하나가 당신의 지시를 세밀하게 수행할 것이다.')\n"
     ]
    }
   ],
   "source": [
    "corpus_textinging2 =  corpus.get_all_texts()\n",
    "print(corpus_textinging2[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('제품과 지리학은 크림 스키밍을 작동시키는 것이다.', '사람들이 기억하면 다음 수준으로 물건을 잃는다.', '우리 팀의 일원이 당신의 명령을 엄청나게 정확하게 실행할 것이다.')\n"
     ]
    }
   ],
   "source": [
    "corpus_textinging =  corpus.get_all_pairs()\n",
    "print(corpus_textinging[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[('사랑해', 0.9141887426376343), ('사랑', 0.8800457715988159), ('사랑한', 0.8743051290512085), ('사랑해.', 0.8486690521240234), ('사랑해서', 0.8482429385185242)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fastText = FastText(\n",
    "    sentences=tokens,\n",
    "    vector_size=128,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,\n",
    "    epochs=3,\n",
    "    min_n=2,\n",
    "    max_n=6\n",
    ")\n",
    "\n",
    "oov_token = \"사랑해요\"\n",
    "oov_vector = fastText.wv[oov_token]\n",
    "\n",
    "print(oov_token in fastText.wv.index_to_key)\n",
    "print(fastText.wv.most_similar(oov_vector, topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 512])\n",
      "torch.Size([6, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "input_size = 128\n",
    "output_size = 256\n",
    "num_layers = 3\n",
    "bidirectional = True\n",
    "\n",
    "model = nn.RNN(\n",
    "    input_size=input_size,\n",
    "    hidden_size=output_size,\n",
    "    num_layers=num_layers,\n",
    "    nonlinearity=\"tanh\",\n",
    "    batch_first=True,\n",
    "    bidirectional=bidirectional,\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "sequence_len = 6\n",
    "\n",
    "inputs = torch.randn(batch_size, sequence_len, input_size)\n",
    "h_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, output_size)\n",
    "\n",
    "outputs, hidden = model(inputs, h_0)\n",
    "print(outputs.shape)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 128])\n",
      "torch.Size([6, 4, 64])\n",
      "torch.Size([6, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "input_size = 128\n",
    "output_size = 256\n",
    "num_layers = 3\n",
    "bidirectional = True\n",
    "proj_size = 64\n",
    "\n",
    "model = nn.LSTM(\n",
    "    input_size = input_size,\n",
    "    hidden_size = output_size,\n",
    "    num_layers = num_layers,\n",
    "    batch_first=True,\n",
    "    bidirectional = bidirectional,\n",
    "    proj_size = proj_size,\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "sequence_len = 6\n",
    "\n",
    "input = torch.randn(batch_size, sequence_len, input_size)\n",
    "h_0 = torch.rand(\n",
    "    num_layers * (int(bidirectional) + 1),\n",
    "    batch_size,\n",
    "    proj_size if proj_size > 0 else output_size,\n",
    ")\n",
    "c_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, output_size)\n",
    "\n",
    "outputs, (h_n, c_n) = model(inputs,(h_0,c_0))\n",
    "\n",
    "print(outputs.shape)\n",
    "print(h_n.shape)\n",
    "print(c_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_vocab,\n",
    "            hidden_dim,\n",
    "            embedding_dim,\n",
    "            n_layers,\n",
    "            dropout=0.5,\n",
    "            bidirectional=True,\n",
    "            model_type=\"lstm\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        if model_type==\"rnn\":\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif model_type==\"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim*2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output,_ = self.model(embeddings)\n",
    "        last_output = output[:,-1,:]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/yeeun/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/yeeun/Korpora/nsmc/ratings_test.txt\n",
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size : 45000\n",
      "Testing Data Size : 5000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus_df = pd.DataFrame(corpus.test)\n",
    "\n",
    "train = corpus_df.sample(frac=0.9, random_state=42)\n",
    "test = corpus_df.drop(train.index)\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print(\"Training Data Size :\", len(train))\n",
    "print(\"Testing Data Size :\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n"
     ]
    }
   ],
   "source": [
    "print(train.head(5).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens = [\"<pad>\",\"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences : \n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "unk_id = token_to_id[\"<unk>\"]\n",
    "train_ids = [[token_to_id.get(token, unk_id) for token in review] for review in train_tokens]\n",
    "test_ids = [[token_to_id.get(token, unk_id) for token in review] for review in test_tokens]\n",
    "\n",
    "max_length = 32\n",
    "pad_id = token_to_id[\"<pad>\"]\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"mps\"\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6937450766563416\n",
      "Train Loss 500 : 0.6937222440323668\n",
      "Train Loss 1000 : 0.6897995989401262\n",
      "Train Loss 1500 : 0.6803381758916386\n",
      "Train Loss 2000 : 0.670381643142419\n",
      "Train Loss 2500 : 0.6596216181548583\n",
      "Val Loss : 0.6010516158308084, Val Accuracy : 0.6972\n",
      "Train Loss 0 : 0.6611526012420654\n",
      "Train Loss 500 : 0.5616417812730023\n",
      "Train Loss 1000 : 0.5662748206566859\n",
      "Train Loss 1500 : 0.5584111838718798\n",
      "Train Loss 2000 : 0.5471742585308846\n",
      "Train Loss 2500 : 0.5328354818708465\n",
      "Val Loss : 0.4439743774386641, Val Accuracy : 0.7842\n",
      "Train Loss 0 : 0.5699650049209595\n",
      "Train Loss 500 : 0.408001613518792\n",
      "Train Loss 1000 : 0.4137545296853477\n",
      "Train Loss 1500 : 0.41021554544319877\n",
      "Train Loss 2000 : 0.4084874798861639\n",
      "Train Loss 2500 : 0.40686385386267077\n",
      "Val Loss : 0.4042650800162611, Val Accuracy : 0.8122\n",
      "Train Loss 0 : 0.4911908507347107\n",
      "Train Loss 500 : 0.3496875149344732\n",
      "Train Loss 1000 : 0.34798683757548565\n",
      "Train Loss 1500 : 0.3564217708245426\n",
      "Train Loss 2000 : 0.355486407756865\n",
      "Train Loss 2500 : 0.35597809166407307\n",
      "Val Loss : 0.40817385635817777, Val Accuracy : 0.8136\n",
      "Train Loss 0 : 0.16961945593357086\n",
      "Train Loss 500 : 0.3073980806919629\n",
      "Train Loss 1000 : 0.3162692142027122\n",
      "Train Loss 1500 : 0.3201902825447379\n",
      "Train Loss 2000 : 0.3210680657445729\n",
      "Train Loss 2500 : 0.32233504646095407\n",
      "Val Loss : 0.4050724592547828, Val Accuracy : 0.823\n"
     ]
    }
   ],
   "source": [
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
    "\n",
    "\n",
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보고싶다 [-1.810407    1.8169253  -0.48161906  1.5159713   0.8511734  -1.5590827\n",
      " -0.6756798  -1.3133336   0.73325855 -1.7288445   0.33666036  1.3928654\n",
      "  0.9024288  -0.28784406  0.04973656  0.18989508  0.20420694  0.07493209\n",
      "  0.3572883  -0.14774302 -1.3679627  -0.13677141  1.0991653  -1.2759937\n",
      "  1.0168123  -0.63305974  0.69073355 -0.5054359   0.50896364  0.9338197\n",
      "  1.5187514  -0.57143223 -0.6127213  -0.02596788 -0.37887442 -0.09916769\n",
      "  0.25106806 -0.6080216  -0.3349899  -1.0512891   0.22265625 -0.13156839\n",
      "  0.06657961  0.9582734   0.582223   -1.6200464  -0.7271793   0.9927755\n",
      " -2.0320892  -0.8715797   1.5874903   0.03204614  1.9326252   0.16329236\n",
      "  0.90480226  0.53742045 -1.011023    1.0754802   0.70510226 -2.1712105\n",
      " -1.6071408   0.32887653  0.4411895  -0.7223813  -0.5257365   0.61660343\n",
      "  0.5548619   1.4530944  -1.7684764  -1.6854795   0.59636015  1.1337516\n",
      " -0.9534812  -0.49850422 -1.4196064  -0.5731302  -1.8976761  -0.62705547\n",
      " -1.3231949   0.26690468  0.3556816   0.4751386   0.96977586  1.453001\n",
      "  0.65167916  0.70896727  1.7263129  -0.5409109   0.7017077  -0.6229258\n",
      " -1.0024966   0.18060283 -0.36892477  0.59494513  0.3632139   1.3933461\n",
      " -0.7412162   2.1945384  -0.22143033  1.656691    0.03344129 -1.1009587\n",
      "  0.07771823  0.15355365  0.98333216 -0.28823376 -1.0350773  -1.0274717\n",
      " -0.9574466  -0.8465707   0.38411048 -1.8725096   2.662399    0.9892227\n",
      "  0.16036855 -0.5830326  -1.712213   -2.3033588  -0.8939816  -0.2199159\n",
      "  1.4724623   0.2978358   0.06200976  0.24875566  1.3845166  -0.14655833\n",
      "  0.36251006 -0.4967541 ]\n"
     ]
    }
   ],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = emb\n",
    "\n",
    "token = vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../word2vec.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb 셀 29\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m word2vec \u001b[39m=\u001b[39m Word2Vec\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m../word2vec.model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m init_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((n_vocab, embedding_dim))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, token \u001b[39min\u001b[39;00m id_to_token\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:1942\u001b[0m, in \u001b[0;36mWord2Vec.load\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \n\u001b[1;32m   1925\u001b[0m \u001b[39mSee Also\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1939\u001b[0m \n\u001b[1;32m   1940\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1942\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(Word2Vec, \u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39mload(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1943\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model, Word2Vec):\n\u001b[1;32m   1944\u001b[0m         rethrow \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/utils.py:486\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    482\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m object from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, fname)\n\u001b[1;32m    484\u001b[0m compress, subname \u001b[39m=\u001b[39m SaveLoad\u001b[39m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[0;32m--> 486\u001b[0m obj \u001b[39m=\u001b[39m unpickle(fname)\n\u001b[1;32m    487\u001b[0m obj\u001b[39m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n\u001b[1;32m    488\u001b[0m obj\u001b[39m.\u001b[39madd_lifecycle_event(\u001b[39m\"\u001b[39m\u001b[39mloaded\u001b[39m\u001b[39m\"\u001b[39m, fname\u001b[39m=\u001b[39mfname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/utils.py:1460\u001b[0m, in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munpickle\u001b[39m(fname):\n\u001b[1;32m   1447\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \n\u001b[1;32m   1449\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \n\u001b[1;32m   1459\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1460\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(fname, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m   1461\u001b[0m         \u001b[39mreturn\u001b[39;00m _pickle\u001b[39m.\u001b[39mload(f, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/smart_open/smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 188\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[1;32m    189\u001b[0m     uri,\n\u001b[1;32m    190\u001b[0m     mode,\n\u001b[1;32m    191\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[1;32m    192\u001b[0m     buffering\u001b[39m=\u001b[39mbuffering,\n\u001b[1;32m    193\u001b[0m     encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m    194\u001b[0m     errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    195\u001b[0m     newline\u001b[39m=\u001b[39mnewline,\n\u001b[1;32m    196\u001b[0m )\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/smart_open/smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    359\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[0;32m--> 361\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39mbuffering, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../word2vec.model'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec.load(\"../word2vec.model\")\n",
    "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
    "\n",
    "for index, token in id_to_token.items():\n",
    "    if token not in [\"<pad>\", \"<unk>\"]:\n",
    "        init_embeddings[index] = word2vec.wv[token]\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb 셀 31\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m optim\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X64sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m classifier \u001b[39m=\u001b[39m SentenceClassifier(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     n_vocab\u001b[39m=\u001b[39mn_vocab, hidden_dim\u001b[39m=\u001b[39mhidden_dim, embedding_dim\u001b[39m=\u001b[39membedding_dim, \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X64sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m n_layers\u001b[39m=\u001b[39mn_layers, pretrained_embedding\u001b[39m=\u001b[39minit_embeddings\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X64sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X64sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X64sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mRMSprop(classifier\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, \n",
    "n_layers=n_layers, pretrained_embedding=init_embeddings\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_embedding, filter_sizes, max_length, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretained(\n",
    "            torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
    "        )\n",
    "        embedding_dim = self.embedding.weight.shape[1]\n",
    "\n",
    "        conv = []\n",
    "        for size in filter_sizes:\n",
    "            conv.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=embedding_dim,\n",
    "                        out_channels=1,\n",
    "                        kernel_size=size\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool1d(kernel_size=max_length-size-1),\n",
    "                )\n",
    "            )\n",
    "        self.conv_filters = nn.ModuleList(conv)\n",
    "\n",
    "        output_size = len(filter_sizes)\n",
    "        self.pre_classifier = nn.Linear(output_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(output_size, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "\n",
    "        conv_outputs   = [conv(embeddings) for conv in self.conv_filters]\n",
    "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
    "\n",
    "        logits = self.pre_classifier(concat_outputs)\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.classifier(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/yeeun/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/yeeun/Korpora/nsmc/ratings_test.txt\n",
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size : 45000\n",
      "Testing Data Size : 5000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpus_df = pd.DataFrame(corpus.test)\n",
    "\n",
    "train = corpus_df.sample(frac=0.9, random_state=42)\n",
    "test = corpus_df.drop(train.index)\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print(\"Training Data Size :\", len(train))\n",
    "print(\"Testing Data Size :\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens\n",
    "    for token, count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens = [\"<pad>\",\"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences : \n",
    "        sequence = sequence[:max_length]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "unk_id = token_to_id[\"<unk>\"]\n",
    "train_ids = [[token_to_id.get(token, unk_id) for token in review] for review in train_tokens]\n",
    "test_ids = [[token_to_id.get(token, unk_id) for token in review] for review in test_tokens]\n",
    "\n",
    "max_length = 32\n",
    "pad_id = token_to_id[\"<pad>\"]\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SentenceClassifier.__init__() got an unexpected keyword argument 'n_vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb 셀 36\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X50sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m n_layers \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mmps\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m classifier \u001b[39m=\u001b[39m SentenceClassifier(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     n_vocab\u001b[39m=\u001b[39mn_vocab, hidden_dim\u001b[39m=\u001b[39mhidden_dim, embedding_dim\u001b[39m=\u001b[39membedding_dim, n_layers\u001b[39m=\u001b[39mn_layers\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mRMSprop(classifier\u001b[39m.\u001b[39mparameters(),lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: SentenceClassifier.__init__() got an unexpected keyword argument 'n_vocab'"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"mps\"\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb 셀 37\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X51sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m interval \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X51sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X51sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     train(classifier, train_loader, criterion, optimizer, device, interval)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X51sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     test(classifier, test_loader, criterion, device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
    "\n",
    "\n",
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb 셀 38\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m token_to_embedding \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embedding_matrix \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39membedding\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m word, emb \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(vocab, embedding_matrix):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     token_to_embedding[word] \u001b[39m=\u001b[39m emb\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = emb\n",
    "\n",
    "token = vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../models/word2vec.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb 셀 39\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m word2vec \u001b[39m=\u001b[39m Word2Vec\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m../models/word2vec.model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m init_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((n_vocab, embedding_dim))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap6.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, token \u001b[39min\u001b[39;00m id_to_token\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:1942\u001b[0m, in \u001b[0;36mWord2Vec.load\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \n\u001b[1;32m   1925\u001b[0m \u001b[39mSee Also\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1939\u001b[0m \n\u001b[1;32m   1940\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1942\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(Word2Vec, \u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39mload(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1943\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model, Word2Vec):\n\u001b[1;32m   1944\u001b[0m         rethrow \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/utils.py:486\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    482\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m object from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, fname)\n\u001b[1;32m    484\u001b[0m compress, subname \u001b[39m=\u001b[39m SaveLoad\u001b[39m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[0;32m--> 486\u001b[0m obj \u001b[39m=\u001b[39m unpickle(fname)\n\u001b[1;32m    487\u001b[0m obj\u001b[39m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n\u001b[1;32m    488\u001b[0m obj\u001b[39m.\u001b[39madd_lifecycle_event(\u001b[39m\"\u001b[39m\u001b[39mloaded\u001b[39m\u001b[39m\"\u001b[39m, fname\u001b[39m=\u001b[39mfname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/utils.py:1460\u001b[0m, in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munpickle\u001b[39m(fname):\n\u001b[1;32m   1447\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \n\u001b[1;32m   1449\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \n\u001b[1;32m   1459\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1460\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(fname, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m   1461\u001b[0m         \u001b[39mreturn\u001b[39;00m _pickle\u001b[39m.\u001b[39mload(f, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/smart_open/smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 188\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[1;32m    189\u001b[0m     uri,\n\u001b[1;32m    190\u001b[0m     mode,\n\u001b[1;32m    191\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[1;32m    192\u001b[0m     buffering\u001b[39m=\u001b[39mbuffering,\n\u001b[1;32m    193\u001b[0m     encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m    194\u001b[0m     errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    195\u001b[0m     newline\u001b[39m=\u001b[39mnewline,\n\u001b[1;32m    196\u001b[0m )\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/smart_open/smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    359\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[0;32m--> 361\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39mbuffering, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../models/word2vec.model'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec.load(\"../models/word2vec.model\")\n",
    "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
    "\n",
    "for index, token in id_to_token.items():\n",
    "    if token not in [\"<pad>\", \"<unk>\"]:\n",
    "        init_embeddings[index] = word2vec.wv[token]\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.cuda.is_available() else \"cpu\"\n",
    "filter_sizes = [3, 3, 4, 4, 5, 5]\n",
    "classifier = SentenceClassifier(\n",
    "    pretrained\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
