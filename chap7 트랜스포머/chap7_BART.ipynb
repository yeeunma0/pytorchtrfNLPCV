{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/yeeun/.cache/huggingface/datasets/argilla___parquet/argilla--news-summary-46ccad7a40bceec1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source News : DANANG, Vietnam (Reuters) - Russian President Vladimir Putin said on Saturday he had a normal dialogue with U.S. leader Donald Trump at a summit in Vietnam, and described Trump as civil, well-educated\n",
      "Summarization : Putin says had useful interaction with Trump at Vi\n",
      "Training Data Size : 3000\n",
      "Validation Data Size : 1000\n",
      "Tesing Data Size : 1000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"뉴스 요약 데이터세트 불러오기\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "news = load_dataset(\"argilla/news-summary\",split=\"test\")\n",
    "df = news.to_pandas().sample(5000, random_state=42)[[\"text\",\"prediction\"]]\n",
    "df[\"prediction\"] = df[\"prediction\"].map(lambda x: x[0][\"text\"])\n",
    "train, valid, test = np.split(\n",
    "    df.sample(frac=1, random_state=42), [int(0.6*(len(df))), int(0.8*len(df))]\n",
    ")\n",
    "\n",
    "print(f\"Source News : {train.text.iloc[0][:200]}\")\n",
    "print(f\"Summarization : {train.prediction.iloc[0][:50]}\")\n",
    "print(f\"Training Data Size : {len(train)}\")\n",
    "print(f\"Validation Data Size : {len(valid)}\")\n",
    "print(f\"Tesing Data Size : {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18006</th>\n",
       "      <td>WASHINGTON (Reuters) - President Barack Obama ...</td>\n",
       "      <td>Obama did not indicate preference for Democrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16987</th>\n",
       "      <td>BEIJING (Reuters) - Chinese educational servic...</td>\n",
       "      <td>China's RYB Education fires head of Beijing ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6586</th>\n",
       "      <td>CARACAS (Reuters) - Venezuelan President Nicol...</td>\n",
       "      <td>Venezuela's Maduro approval rises to 23 percen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14737</th>\n",
       "      <td>JOHANNESBURG (Reuters) - Zimbabweans living in...</td>\n",
       "      <td>Zimbabweans in South Africa hope for change at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>Karangasem, INDONESIA (Reuters) - Fears that a...</td>\n",
       "      <td>Bali's rumbling volcano spurs travel warnings ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15547</th>\n",
       "      <td>JOHANNESBURG (Reuters) - South Africa s ruling...</td>\n",
       "      <td>South Africa's ANC needs to put an end to scan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3855</th>\n",
       "      <td>WASHINGTON (Reuters) - President Donald Trump ...</td>\n",
       "      <td>Despite recusal, Trump has confidence in Sessi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9734</th>\n",
       "      <td>WASHINGTON/UNITED NATIONS (Reuters) - U.S. Pre...</td>\n",
       "      <td>Trump threatens to cut aid to U.N. members ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4113</th>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>Trump's call for military buildup hits bump in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8614</th>\n",
       "      <td>WASHINGTON (Reuters) - Michigan Governor Rick ...</td>\n",
       "      <td>Michigan governor, EPA to testify at House pan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "18006  WASHINGTON (Reuters) - President Barack Obama ...   \n",
       "16987  BEIJING (Reuters) - Chinese educational servic...   \n",
       "6586   CARACAS (Reuters) - Venezuelan President Nicol...   \n",
       "14737  JOHANNESBURG (Reuters) - Zimbabweans living in...   \n",
       "856    Karangasem, INDONESIA (Reuters) - Fears that a...   \n",
       "...                                                  ...   \n",
       "15547  JOHANNESBURG (Reuters) - South Africa s ruling...   \n",
       "3855   WASHINGTON (Reuters) - President Donald Trump ...   \n",
       "9734   WASHINGTON/UNITED NATIONS (Reuters) - U.S. Pre...   \n",
       "4113   WASHINGTON (Reuters) - U.S. President Donald T...   \n",
       "8614   WASHINGTON (Reuters) - Michigan Governor Rick ...   \n",
       "\n",
       "                                              prediction  \n",
       "18006  Obama did not indicate preference for Democrat...  \n",
       "16987  China's RYB Education fires head of Beijing ki...  \n",
       "6586   Venezuela's Maduro approval rises to 23 percen...  \n",
       "14737  Zimbabweans in South Africa hope for change at...  \n",
       "856    Bali's rumbling volcano spurs travel warnings ...  \n",
       "...                                                  ...  \n",
       "15547  South Africa's ANC needs to put an end to scan...  \n",
       "3855   Despite recusal, Trump has confidence in Sessi...  \n",
       "9734   Trump threatens to cut aid to U.N. members ove...  \n",
       "4113   Trump's call for military buildup hits bump in...  \n",
       "8614   Michigan governor, EPA to testify at House pan...  \n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([   0,  495, 1889,  ...,    1,    1,    1], device='mps:0'), tensor([1, 1, 1,  ..., 0, 0, 0], device='mps:0'), tensor([    0, 35891,   161,    56,  5616, 10405,    19,   140,    23,  5490,\n",
      "         3564,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "       device='mps:0'))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"BART 입력 텐서 생성\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import BartTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def make_dataset(data, tokenizer, device):\n",
    "    tokenized = tokenizer(\n",
    "        text = data.text.tolist(),\n",
    "        padding = \"longest\",\n",
    "        truncation = True,\n",
    "        return_tensors = \"pt\"\n",
    "    )\n",
    "    labels = []\n",
    "    input_ids = tokenized[\"input_ids\"].to(device)\n",
    "    attention_mask = tokenized[\"attention_mask\"].to(device)\n",
    "    for target in data.prediction:\n",
    "        labels.append(tokenizer.encode(target, return_tensors=\"pt\").squeeze())\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100).to(device) #교차엔트로피와 같은 손실함수에서 패딩된 토큰을 무시하게 하기 위해 사용\n",
    "    return TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "def get_dataloader(dataset, sampler, batch_size):\n",
    "    data_sampler = sampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 8\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "tokenizer = BartTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"facebook/bart-base\"\n",
    ")\n",
    "\n",
    "train_dataset = make_dataset(train, tokenizer, device)\n",
    "train_dataloader = get_dataloader(train_dataset, RandomSampler, batch_size) #무작위로 샘플링\n",
    "\n",
    "valid_dataset = make_dataset(train, tokenizer, device)\n",
    "valid_dataloader = get_dataloader(train_dataset, SequentialSampler, batch_size) #고정된 순서대로 샘플링\n",
    "\n",
    "test_dataset = make_dataset(train, tokenizer, device)\n",
    "test_dataloader = get_dataloader(train_dataset, SequentialSampler, batch_size)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BART 모델 선언\"\"\"\n",
    "\n",
    "from torch import optim\n",
    "from transformers import BartForConditionalGeneration #조건부 생성 클래스\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"facebook/bart-base\" #빠른 학습을 위해 6개의 계층을 사용하는 bart-base 사용, 12개면 bart-large\n",
    ").to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "└ shared\n",
      "└ encoder\n",
      "│  └ embed_tokens\n",
      "│  └ embed_positions\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  └ layernorm_embedding\n",
      "└ decoder\n",
      "│  └ embed_tokens\n",
      "│  └ embed_positions\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  └ layernorm_embedding\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "\"\"\"BART 모델 구조 출력\"\"\"\n",
    "\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(pretrained_model_name_or_path=\"facebook/bart-base\").to(device)\n",
    "#사전 학습된 모델(pretrained_model_name_or_path)을 from_pretrained 메서드로 불러옴\n",
    "\n",
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\",sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│  └\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                print(\"│  │  └\",sssub_name)\n",
    "\n",
    "# shared 계층은 인코더와 디코더가 공유하는 임베딩 계층 -> 이러한 공유로 인코더와 디코더 간의 연결을 강화\n",
    "# 인코더의 마지막 계층의 출력값은 디코더의 모든 계층과 어텐션 연산을 수행\n",
    "# 마지막 디코더 계층의 출력값은 출력 크기가 단어 사전의 크기인 완전 연결 계층을 통과해 언어 모델을 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BART 모델 학습 및 검증\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def calc_rouge(preds, labels):\n",
    "    preds = preds.argmax(axis=-1)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    rouge2 = rouge_score.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels\n",
    "    )\n",
    "    return rouge2[\"rouge2\"]\n",
    "\n",
    "\n",
    "def train(model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for input_ids, attention_mask, labels in dataloader:\n",
    "        outputs = model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            labels = labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    return train_loss\n",
    "\n",
    "def evaluation(model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss, val_rouge = 0.0, 0.0\n",
    "\n",
    "        for input_ids, attention_mask, labels in dataloader:\n",
    "            outputs = model(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                labels = labels\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to(\"cpu\").numpy()\n",
    "            rouge = calc_rouge(logits, label_ids)\n",
    "\n",
    "            val_loss += loss\n",
    "            val_rouge += rouge\n",
    "\n",
    "    val_loss = val_loss/len(dataloader)\n",
    "    val_rouge = val_rouge/len(dataloader)\n",
    "    return val_loss, val_rouge\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\", tokenizer=tokenizer)\n",
    "\n",
    "best_loss = 10000\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, optimizer, train_dataloader)\n",
    "    val_loss, val_accuracy = evaluation(model, valid_dataloader)\n",
    "    print(f\"Epoch:{epoch+1}, Train loss:{train_loss: .4f}, Val loss : {val_loss:.4f}, Val accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(),\"models/BartForConditionalGeneration.pt\")\n",
    "        print(\"Saved the model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BART 모델 평가\"\"\"\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"facebook.bart-base\"\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"models/BartForConditionalGeneration.pt\"))\n",
    "\n",
    "test_loss,test_rouge_score = evaluation(model, test_dataloader)\n",
    "print(f\"Test Loss : {test_loss : .4f}\")\n",
    "print(f\"Test ROUGE-2 Score : {test_rouge_score : .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"문장 요약문 비교\"\"\"\n",
    "from transformers import Pipeline\n",
    "\n",
    "summarizer = Pipeline(\n",
    "    task = \"summarization\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 54,\n",
    "    device = \"cpu\"\n",
    ")\n",
    "\n",
    "for index in range(5):\n",
    "    news_text = test.text.iloc[index]\n",
    "    summarization = test.prediction.iloc[index]\n",
    "    prediction_summarization = summarizer(news_text)[0][\"summary_text\"]\n",
    "    print(f\"정답 요약문 : {summarization}\")\n",
    "    print(f\"모델 요약문 : {prediction_summarization}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
