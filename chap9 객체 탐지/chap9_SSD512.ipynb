{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD512 특징 추출 네트워크 정의\n",
    "\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class SSDBackbone(nn.Module):\n",
    "    def __init__(self,backbone):\n",
    "        # Resnet34를 백본으로 사용하며, 입력 줄기(layer0)와 4개의 스테이지(layer1~layer4)로 구성\n",
    "        super().__init__()\n",
    "        layer0 = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu)\n",
    "        layer1 = backbone.layer1\n",
    "        layer2 = backbone.layer2\n",
    "        layer3 = backbone.layer3\n",
    "        layer4 = backbone.layer4\n",
    "\n",
    "        # 세 번째 스테이지에서 분기를 나눠야하므로 세 번째 스테이지까지 features로 정의\n",
    "        self.features = nn.Sequential(layer0, layer1, layer2, layer3)\n",
    "        # 이후 upsampling계층으로 추출된 특징 맵의 차원 수를 늘림\n",
    "        self.upsampling = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # Resnet-34의 마지막 계층에 연결하는 블록으로 멀티 스케일 특징 맵을 추출하는 계츨들로 구성\n",
    "        self.extra = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    layer4,\n",
    "                    nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1),\n",
    "                    nn.ReLU(inplace=True), # 입력 텐서의 직접 수정 여부(True이면 텐서를 직접 수정해 출력을 생성하지 않으므로 메모리 사용량이 줄어듬)\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(1024, 256, kernel_size=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(512, 128, kernel_size=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(256, 128, kernel_size=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(128, 256, kernel_size=3),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(256, 128, kernel_size=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(128, 256, kernel_size=3),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(256, 128, kernel_size=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(128, 256, kernel_size=3),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    # 앞에서 정의한 계층들을 순서대로 연결해 순전파를 수행\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        output = [self.upsampling(x)]\n",
    "\n",
    "        for block in self.extra:\n",
    "            x = block(x)\n",
    "            output.append(x)\n",
    "\n",
    "        return OrderedDict([(str(i),v) for i, v in enumerate(output)]) \n",
    "        # 순서를 보장하는 딕셔너리로 변환해 반환, 이 딕셔너리는 클래스 분류 및 박스 회귀 네트워크에 전달됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD512 모델 생성\n",
    "\n",
    "import torch\n",
    "from torchvision.models import resnet34\n",
    "from torchvision.models.detection import ssd\n",
    "from torchvision.models.detection.anchor_utils import DefaultBoxGenerator\n",
    "\n",
    "backbone_base = resnet34(weights = \"ResNet34_Weights.IMAGENET1K_V1\")\n",
    "backbone = SSDBackbone(backbone_base) # 사전학습된 모델을 불러와 SSDBackbone에 저장\n",
    "anchor_generator = DefaultBoxGenerator(\n",
    "    aspect_ratios=[[2], [2,3], [2,3], [2,3], [2,3], [2], [2]], # 기본 박스 종횡비([2]는 1:2인 기본박스 생성, [2,3]은 1:2,1:3인 기본박스 생성)\n",
    "    #여기에 왜 1:! 기본박스는 없지?\n",
    "    scales=[0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05, 1.20], # 기본 박스 비율(생성할 기본 박스의 크기 설정. 값이 클 수록 큰 박스)\n",
    "    steps = [8, 16, 32, 64, 100, 300, 512], # 기본 박스 간격(기본 박스의 다운 샘플링 비율)\n",
    ")\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "model = ssd.SSD(\n",
    "    backbone=backbone,\n",
    "    anchor_generator=anchor_generator,\n",
    "    size=(512,512),\n",
    "    num_classes=3\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root, train, transform=None):\n",
    "        super().__init__()\n",
    "        directory = \"train\" if train else \"val\"\n",
    "        annotations = os.path.join(root, \"annotations\", f\"{directory}_annotations.json\")\n",
    "        \n",
    "        self.coco = COCO(annotations)\n",
    "        self.iamge_path = os.path.join(root, directory)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.categories = self._get_categories()\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _get_categories(self):\n",
    "        categories = {0: \"background\"}\n",
    "        for category in self.coco.cats.values():\n",
    "            categories[category[\"id\"]] = category[\"name\"]\n",
    "        return categories\n",
    "    \n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "        for _id in self.coco.imgs:\n",
    "            file_name = self.coco.loadImgs(_id)[0][\"file_name\"]\n",
    "            image_path = os.path.join(self.iamge_path, file_name)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            anns = self.coco.loadAnns(self.coco.getAnnIds(_id))\n",
    "            for ann in anns:\n",
    "                x, y, w, h = ann[\"bbox\"]\n",
    "                \n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "                labels.append(ann[\"category_id\"])\n",
    "\n",
    "            target = {\n",
    "            \"image_id\": torch.LongTensor([_id]),\n",
    "                \"boxes\": torch.FloatTensor(boxes),\n",
    "                \"labels\": torch.LongTensor(labels)\n",
    "            }\n",
    "            data.append([image, target])\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, target = self.data[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collator(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(dtype=torch.float)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = COCODataset(\"../datasets/coco\", train=True, transform=transform)\n",
    "test_dataset = COCODataset(\"../datasets/coco\", train=False, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=collator\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=True, drop_last=True, collate_fn=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1, Cost : nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yeeun/Desktop/yeeunn/NLPCV/chap9 객체 탐지/chap9_SSD512.ipynb 셀 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap9%20%EA%B0%9D%EC%B2%B4%20%ED%83%90%EC%A7%80/chap9_SSD512.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap9%20%EA%B0%9D%EC%B2%B4%20%ED%83%90%EC%A7%80/chap9_SSD512.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     losses\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap9%20%EA%B0%9D%EC%B2%B4%20%ED%83%90%EC%A7%80/chap9_SSD512.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap9%20%EA%B0%9D%EC%B2%B4%20%ED%83%90%EC%A7%80/chap9_SSD512.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     cost \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m losses\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap9%20%EA%B0%9D%EC%B2%B4%20%ED%83%90%EC%A7%80/chap9_SSD512.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/sgd.py:75\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     71\u001b[0m momentum_buffer_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m has_sparse_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 75\u001b[0m sgd(params_with_grad,\n\u001b[1;32m     76\u001b[0m     d_p_list,\n\u001b[1;32m     77\u001b[0m     momentum_buffer_list,\n\u001b[1;32m     78\u001b[0m     weight_decay\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     79\u001b[0m     momentum\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mmomentum\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     80\u001b[0m     lr\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     81\u001b[0m     dampening\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mdampening\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     82\u001b[0m     nesterov\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mnesterov\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     83\u001b[0m     maximize\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     84\u001b[0m     has_sparse_grad\u001b[39m=\u001b[39mhas_sparse_grad,\n\u001b[1;32m     85\u001b[0m     foreach\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mforeach\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     87\u001b[0m \u001b[39m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m p, momentum_buffer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/sgd.py:220\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 220\u001b[0m func(params,\n\u001b[1;32m    221\u001b[0m      d_p_list,\n\u001b[1;32m    222\u001b[0m      momentum_buffer_list,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39mweight_decay,\n\u001b[1;32m    224\u001b[0m      momentum\u001b[39m=\u001b[39mmomentum,\n\u001b[1;32m    225\u001b[0m      lr\u001b[39m=\u001b[39mlr,\n\u001b[1;32m    226\u001b[0m      dampening\u001b[39m=\u001b[39mdampening,\n\u001b[1;32m    227\u001b[0m      nesterov\u001b[39m=\u001b[39mnesterov,\n\u001b[1;32m    228\u001b[0m      has_sparse_grad\u001b[39m=\u001b[39mhas_sparse_grad,\n\u001b[1;32m    229\u001b[0m      maximize\u001b[39m=\u001b[39mmaximize)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/sgd.py:247\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    244\u001b[0m d_p \u001b[39m=\u001b[39m d_p_list[i] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m maximize \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39md_p_list[i]\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 247\u001b[0m     d_p \u001b[39m=\u001b[39m d_p\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m momentum \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    250\u001b[0m     buf \u001b[39m=\u001b[39m momentum_buffer_list[i]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    cost = 0.0\n",
    "    for idx, (images, targets) in enumerate(train_dataloader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += losses\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    cost = cost / len(train_dataloader)\n",
    "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "\n",
    "def draw_bbox(ax, box, text, color):\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle(\n",
    "            xy=(box[0], box[1]),\n",
    "            width=box[2] - box[0],\n",
    "            height=box[3] - box[1],\n",
    "            fill=False,\n",
    "            edgecolor=color,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    )\n",
    "    ax.annotate(\n",
    "        text=text,\n",
    "        xy=(box[0] - 5, box[1] - 5),\n",
    "        color=color,\n",
    "        weight=\"bold\",\n",
    "        fontsize=13,\n",
    "    )\n",
    " \n",
    "threshold = 0.5\n",
    "categories = test_dataset.categories\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for images, targets in test_dataloader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        outputs = model(images)\n",
    "        \n",
    "        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()\n",
    "        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()\n",
    "        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()\n",
    "        \n",
    "        boxes = boxes[scores >= threshold].astype(np.int32)\n",
    "        labels = labels[scores >= threshold]\n",
    "        scores = scores[scores >= threshold]\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        plt.imshow(to_pil_image(images[0]))\n",
    "\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            draw_bbox(ax, box, f\"{categories[label]} - {score:.4f}\", \"red\")\n",
    "\n",
    "        tboxes = targets[0][\"boxes\"].numpy()\n",
    "        tlabels = targets[0][\"labels\"].numpy()\n",
    "        for box, label in zip(tboxes, tlabels):\n",
    "            draw_bbox(ax, box, f\"{categories[label]}\", \"blue\")\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    coco_detections = []\n",
    "    for images, targets in test_dataloader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "\n",
    "        for i in range(len(targets)):\n",
    "            image_id = targets[i][\"image_id\"].data.cpu().numpy().tolist()[0]\n",
    "            boxes = outputs[i][\"boxes\"].data.cpu().numpy()\n",
    "            boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "            scores = outputs[i][\"scores\"].data.cpu().numpy()\n",
    "            labels = outputs[i][\"labels\"].data.cpu().numpy()\n",
    "\n",
    "            for instance_id in range(len(boxes)):\n",
    "                box = boxes[instance_id, :].tolist()\n",
    "                prediction = np.array(\n",
    "                    [\n",
    "                        image_id,\n",
    "                        box[0],\n",
    "                        box[1],\n",
    "                        box[2],\n",
    "                        box[3],\n",
    "                        float(scores[instance_id]),\n",
    "                        int(labels[instance_id]),\n",
    "                    ]\n",
    "                )\n",
    "                coco_detections.append(prediction)\n",
    "    coco_detections = np.asarray(coco_detections)\n",
    "\n",
    "    coco_gt = test_dataloader.dataset.coco\n",
    "    coco_dt = coco_gt.loadRes(coco_detections)\n",
    "    coco_evaluator = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
    "    coco_evaluator.evaluate()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512, 1024, 512, 256, 256, 256, 256]\n"
     ]
    }
   ],
   "source": [
    "# 출력 채널 할당 방법\n",
    "\n",
    "# 가상의 이미지를 입력해 특성 맵을 추출하고 각 계층의 출력 채널 수를 반환하는 함수\n",
    "def retirieve_out_channels(model, size):\n",
    "    model.eval() # 모델을 평가 모드로 변경\n",
    "    with torch.no_grad():\n",
    "        device = next(model.parameters()).device\n",
    "        image = torch.zeros((1,3,size[1],size[0]),device=device)\n",
    "        features = model(image)\n",
    "\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = OrderedDict([(\"0\",features)])\n",
    "        out_channels = [x.size(1) for x in features.values()]\n",
    "\n",
    "    model.train()\n",
    "    return out_channels\n",
    "\n",
    "print(retirieve_out_channels(backbone,(512,512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
