{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터세트 클래스 선언\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root, train, transform=None): # root:경로, train:train 데이터셋 불러오기(False이면 검증용 불러옴)\n",
    "        super().__init__()\n",
    "        directory = \"train\" if train else \"val\"\n",
    "        annotations = os.path.join(root, \"annotations\", f\"{directory}_annotations.json\") # annotation 디렉터리에 있는 json 파일 경로를 설정\n",
    "\n",
    "        self.coco = COCO(annotations) # 이미지와 어노테이션 정보를 불러오기 전에 학습에 사용되는 카테고리 정보를 불러옴\n",
    "        self.iamge_path = os.path.join(root, directory)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.categories = self._get_categories()\n",
    "        self.data = self._load_data() # 이미지와 어노테이션 정보를 불러옴\n",
    "\n",
    "    # self.coco 인스턴스의 cats 속성에서 카테고리 정보를 불러올 수 있음\n",
    "    # cats 속성은 딕셔너리 구조를 가지며, 상위 카테고리, 카테고리 ID, 카테고리 정보를 포함한다.\n",
    "    def _get_categories(self):\n",
    "        categories = {0: \"background\"} # 모델 추론 시 카테고리 정보를 확인하기 위해 사용\n",
    "        for category in self.coco.cats.values():\n",
    "            categories[category[\"id\"]] = category[\"name\"]\n",
    "        return categories # {0: 'background', 1: 'cat', 2: 'dog'}\n",
    "    \n",
    "    # 각 이미지의 아이디, 박스, 라벨을 갖는 타겟 값과 이미지를 리스트로 반환\n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "        # 어노테이션 JSON 파일의 이미지 정보를 순차적으로 반환, 어노테이션 정보는 이미지 ID와 매핑될 수 있으므로 이미지 ID(_id)를 추출\n",
    "        for _id in self.coco.imgs:\n",
    "            file_name = self.coco.loadImgs(_id)[0][\"file_name\"] # 입력된 이미지 ID를 받아 어노테이션 정보를 반환      \n",
    "            image_path = os.path.join(self.iamge_path, file_name) # 이미지 ID를 받아 이미지의 경로 저장\n",
    "            image = Image.open(image_path).convert(\"RGB\") # 이미지를 불러옴, convert(\"RGB\")는 이미지 데이터를 RGB 색상 모드로 변환\n",
    "\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            # getAnnIds : image ID를 input으로 그에 해당하는 annotation ID를 return, loadAnns() : Annotation Id를 input으로 annotation dict 전체(상세정보)를 return\n",
    "            anns = self.coco.loadAnns(self.coco.getAnnIds(_id)) # json 파일에서 annotations 부분을 불러옴\n",
    "\n",
    "            \"\"\"\"annotations\": [{\"segmentation\": [[374.46, 310.42, 386.68, ..., 376.35, 310.86]], \"area\": 2243.7513000000004, \"iscrowd\": 0, \"image_id\": 495357, \"bbox\": [337.02, 244.46, 66.47, 66.75], \"category_id\": 2, \"id\": 1727},\"\"\"\n",
    "            # annotation 부분에서 bbox와 id를 받아옴\n",
    "            for ann in anns:\n",
    "                x, y, w, h = ann[\"bbox\"] # 왼쪽 상단 모서리(x,y), 너비, 높이\n",
    "                \n",
    "                boxes.append([x, y, x + w, y + h]) # 왼쪽 상단 모서리, 오른쪽 하단 모서리 좌표로 변겨\n",
    "                labels.append(ann[\"category_id\"])\n",
    "\n",
    "            target = {\n",
    "            \"image_id\": torch.LongTensor([_id]),\n",
    "                \"boxes\": torch.FloatTensor(boxes),\n",
    "                \"labels\": torch.LongTensor(labels)\n",
    "            }\n",
    "            data.append([image, target])\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, target = self.data[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# 데이터로더\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 배치 내 이미지를 튜플의 형태로 반환\n",
    "# 예를 들어, 배치 크기가 4이고, 각 데이터 포인트가 이미지와 타겟을 포함한다면, batch는 [(image1, target1), (image2, target2), (image3, target3), (image4, target4)] 형태\n",
    "# zip(*batch)를 실행하면 [(image1, image2, image3, image4), (target1, target2, target3, target4)]와 같이 이미지들과 타겟들이 각각 그룹화\n",
    "def collator(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(dtype=torch.float)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = COCODataset(\"../datasets/coco\", train=True, transform = transform)\n",
    "test_dataset = COCODataset(\"../datasets/coco\", train=False, transform = transform)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=True, drop_last=True, collate_fn=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 백본 및 모델 구조 정의\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import ops\n",
    "from torchvision.models.detection import rpn\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "# 백본 모델은 VGG-16 모델을 사용\n",
    "backbone = models.vgg16(weights=\"VGG16_Weights.IMAGENET1K_V1\").features \n",
    "backbone.out_channels = 512\n",
    "\n",
    "# Faster R-CNN 모델은 2stage\n",
    "# 영역 제안 네트워크\n",
    "anchor_generator = rpn.AnchorGenerator( # 객체 위치 후보군을 생성\n",
    "    sizes=((32, 64, 128, 256, 512),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "# 관심 영역 풀링 : 영역 제안 네트워크에서 생성한 객체 후보군을 입력으로 받아 후보군 내의 특징 맵 영역을 일정한 크기의 고정된 영역으로 샘플링\n",
    "roi_pooler = ops.MultiScaleRoIAlign( # RoI align 기능이 포함되어있음, MultiScaleRoIalign 클래스는 다양한 스케일의 특징 맵을 입력으로 받아, 각 관심 영역 후보군을 해당 스케일의 특징맵에 맞게 샘플링해 고정된 크기의 관심 영역 특징 맵을 생성한다. 이렇게 생성된 특징 맵은 분류 계층의 입력으로 사용된다.\n",
    "    featmap_names=[\"0\"], # VGG의 특징맵은 sequential 형태이다. 따라서 첫 번째 특징 추출 계층을 사용하기 위해 \"0\"으로 설정\n",
    "    output_size=(7, 7),\n",
    "    sampling_ratio=2 # 관심 영역을 샘플링하기 위해 2*2 크기의 그리드를 사용\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "model = FasterRCNN(\n",
    "    backbone=backbone,\n",
    "    num_classes=3,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'background', 1: 'cat', 2: 'dog'}\n"
     ]
    }
   ],
   "source": [
    "# 최적화 함수 및 학습률 스케일러\n",
    "from torch import optim\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad] # 학습이 가능한 매개변수만 params 변수에 저장\n",
    "optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005) # params 변수에 확률적 경사 하강법 적용\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) # 지정된 주기마다 학습률을 감소시켜 학습률이 너무 크게 줄어들거나 작아지는 것을 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[srcBuf length] > 0 INTERNAL ASSERT FAILED at \"/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/OperationUtils.mm\":341, please report a bug to PyTorch. Placeholder tensor is empty!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/yeeun/Desktop/yeeunn/NLPCV/chap9 객체 탐지/chap9_FasterRCNN.ipynb 셀 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap9%20%EA%B0%9D%EC%B2%B4%20%ED%83%90%EC%A7%80/chap9_FasterRCNN.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images) \u001b[39m# 리스트 간소화로 to(device)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap9%20%EA%B0%9D%EC%B2%B4%20%ED%83%90%EC%A7%80/chap9_FasterRCNN.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets] \u001b[39m# 딕셔너리 간소화로 to(device)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap9%20%EA%B0%9D%EC%B2%B4%20%ED%83%90%EC%A7%80/chap9_FasterRCNN.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap9%20%EA%B0%9D%EC%B2%B4%20%ED%83%90%EC%A7%80/chap9_FasterRCNN.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeeun/Desktop/yeeunn/NLPCV/chap9%20%EA%B0%9D%EC%B2%B4%20%ED%83%90%EC%A7%80/chap9_FasterRCNN.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpn(images, features, targets)\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroi_heads(features, proposals, images\u001b[39m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/detection/rpn.py:380\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    378\u001b[0m     labels, matched_gt_boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massign_targets_to_anchors(anchors, targets)\n\u001b[1;32m    379\u001b[0m     regression_targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_coder\u001b[39m.\u001b[39mencode(matched_gt_boxes, anchors)\n\u001b[0;32m--> 380\u001b[0m     loss_objectness, loss_rpn_box_reg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(\n\u001b[1;32m    381\u001b[0m         objectness, pred_bbox_deltas, labels, regression_targets\n\u001b[1;32m    382\u001b[0m     )\n\u001b[1;32m    383\u001b[0m     losses \u001b[39m=\u001b[39m {\n\u001b[1;32m    384\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mloss_objectness\u001b[39m\u001b[39m\"\u001b[39m: loss_objectness,\n\u001b[1;32m    385\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mloss_rpn_box_reg\u001b[39m\u001b[39m\"\u001b[39m: loss_rpn_box_reg,\n\u001b[1;32m    386\u001b[0m     }\n\u001b[1;32m    387\u001b[0m \u001b[39mreturn\u001b[39;00m boxes, losses\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/detection/rpn.py:324\u001b[0m, in \u001b[0;36mRegionProposalNetwork.compute_loss\u001b[0;34m(self, objectness, pred_bbox_deltas, labels, regression_targets)\u001b[0m\n\u001b[1;32m    321\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(labels, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    322\u001b[0m regression_targets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(regression_targets, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 324\u001b[0m box_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msmooth_l1_loss(\n\u001b[1;32m    325\u001b[0m     pred_bbox_deltas[sampled_pos_inds],\n\u001b[1;32m    326\u001b[0m     regression_targets[sampled_pos_inds],\n\u001b[1;32m    327\u001b[0m     beta\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39m9\u001b[39m,\n\u001b[1;32m    328\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    329\u001b[0m ) \u001b[39m/\u001b[39m (sampled_inds\u001b[39m.\u001b[39mnumel())\n\u001b[1;32m    331\u001b[0m objectness_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n\u001b[1;32m    333\u001b[0m \u001b[39mreturn\u001b[39;00m objectness_loss, box_loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3243\u001b[0m, in \u001b[0;36msmooth_l1_loss\u001b[0;34m(input, target, size_average, reduce, reduction, beta)\u001b[0m\n\u001b[1;32m   3241\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39ml1_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n\u001b[1;32m   3242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3243\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39msmooth_l1_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction), beta)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [srcBuf length] > 0 INTERNAL ASSERT FAILED at \"/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/OperationUtils.mm\":341, please report a bug to PyTorch. Placeholder tensor is empty!"
     ]
    }
   ],
   "source": [
    "# Faster R-CNN 미세 조정\n",
    "\n",
    "# Faster R-CNN 학습 과정이라고 생각하면 됨\n",
    "for epoch in range(5):\n",
    "    cost = 0.0\n",
    "    for idx, (images, targets) in enumerate(train_dataloader):\n",
    "        images = list(image.to(device) for image in images) # 리스트 간소화로 to(device)\n",
    "        targets = [{k: v.to(device) for k,v in t.items()} for t in targets] # 딕셔너리 간소화로 to(device)\n",
    "\n",
    "        loss_dict = model(images, targets) # lossclassifier, lossboxreg, lossobjectness, lossrpnboxreg 총 4개의 손실 값이 나옴\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += losses\n",
    "\n",
    "    lr_scheduler.step() # 모든 배치를 처리 후에 스케쥴러를 한 단계 업데이트\n",
    "    cost = cost / len(train_dataloader)\n",
    "    print(f\"Epoch : {epoch+1 : 4d}, Cost : {cost : .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 추론 및 시각화\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def draw_bbox(ax, box, text, color):\n",
    "    ax.add_path(\n",
    "        plt.Rectangle(\n",
    "            xy=(box[0],box[1]),\n",
    "            width=box[2]-box[0],\n",
    "            height=box[3]-box[1],\n",
    "            fill=False,\n",
    "            edgecolor=color,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    )\n",
    "    ax.annotate(\n",
    "        text=text,\n",
    "        xy=(box[0]-5, box[1]-5),\n",
    "        color=color,\n",
    "        weight=\"bold\",\n",
    "        fontsize=13,\n",
    "    )\n",
    "\n",
    "threshold = 0.5\n",
    "categories = test_dataset.categories # {0: 'background', 1: 'cat', 2: 'dog'}\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for images, targets in test_dataloader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        outputs = model(images)\n",
    "        ###outputs = [{'boxes': tensor([[x1, y1, x2, y2], ...]), 'labels': tensor([label1, label2, ...]), 'scores': tensor([score1, score2, ...]) # 각 탐지에 대한 신뢰도 점수}]\n",
    "\n",
    "        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()\n",
    "        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()\n",
    "        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()\n",
    "\n",
    "        boxes = boxes[scores>=threshold].astype(np.int32)\n",
    "        labels = labels[scores>=threshold]\n",
    "        scores = scores[scores>=threshold]\n",
    "\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        plt.imshow(to_pil_image(images[0]))\n",
    "\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            draw_bbox(ax, box, f\"{categories[label]}-{score:.4f}\",\"red\")\n",
    "\n",
    "        tboxes = targets[0][\"boxes\"].numpy()\n",
    "        tlabels = targets[0][\"labels\"].numpy()\n",
    "        for box, labels in zip(tboxes, tlabels):\n",
    "            draw_bbox(ax, box, f\"{categories[label]}\",\"blue\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision(정밀도) : TP/(TP+FP), 모델이 참이라고 예측한 것 중 얼마나 맞았는지\n",
    "\n",
    "Recall(재현율) : TP/(TP+FN), 모델이 실제 참값인 데이터를 얼마나 잘 찾아냈는지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "\n",
    "import numpy as np\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    coco_detections = []\n",
    "    for images, targets in test_dataloader : \n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "\n",
    "        for i in range(len(targets)):\n",
    "            image_id = targets[i][\"image_id\"].data.cpu().numpy().tolist()[0]\n",
    "            boxes = outputs[i][\"boxes\"].data.cpu().numpy()\n",
    "            boxes[:,2] = boxes[:,2]-boxes[:,0]\n",
    "            boxes[:,3] = boxes[:,3]-boxes[:,1]\n",
    "            scores = outputs[i][\"scores\"].data.cpu().numpy()\n",
    "            labels = outputs[i][\"labels\"].data.cpu().numpy()\n",
    "\n",
    "            for instance_id in range(len(boxes)):\n",
    "                box = boxes[instance_id, :].tolist()\n",
    "                prediction = np.array(\n",
    "                    [\n",
    "                        image_id,\n",
    "                        box[0],\n",
    "                        box[1],\n",
    "                        box[2],\n",
    "                        box[3],\n",
    "                        float(scores[instance_id]),\n",
    "                        int(labels[instance_id])\n",
    "                    ]\n",
    "                )\n",
    "                coco_detections.append(prediction)\n",
    "\n",
    "    coco_detections = np.asarray(coco_detections)\n",
    "    coco_gt = test_dataloader.dataset.coco_detections\n",
    "    coco_dt = coco_gt.loadRes(coco_detections)\n",
    "    coco_evaluator = COCOeval(coco_gt, coco_dt, iouTypr=\"bbox\")\n",
    "    coco_evaluator.evaluate()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
